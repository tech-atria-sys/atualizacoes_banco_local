{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import text\n",
    "\n",
    "# --- Configuração dos Módulos ---\n",
    "sys.path.insert(0, r'C:\\Scripts\\modules_azure\\database')\n",
    "sys.path.insert(0, r'C:\\Scripts\\modules_azure\\parameters')\n",
    "\n",
    "from connection_azure import Connect\n",
    "from azure_loader import AzureLoader\n",
    "from parametros import Parametros\n",
    "\n",
    "# --- Configurações Globais ---\n",
    "SCHEMA_DEFAULT = \"dbo\"\n",
    "DIRETORIO_RELATORIOS = r\"C:\\Scripts\\relatórios\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Encontradas 37 pastas. Iniciando leitura...\n",
      "   -> Leitura concluída: 37 dias processados.\n"
     ]
    }
   ],
   "source": [
    "pastas_dias = []\n",
    "if os.path.exists(DIRETORIO_RELATORIOS):\n",
    "    # Pega apenas pastas que são datas válidas\n",
    "    for nome_item in os.listdir(DIRETORIO_RELATORIOS):\n",
    "        caminho_completo = os.path.join(DIRETORIO_RELATORIOS, nome_item)\n",
    "        if os.path.isdir(caminho_completo):\n",
    "            try:\n",
    "                dt = datetime.strptime(nome_item, \"%d-%m-%Y\")\n",
    "                pastas_dias.append({\"dia\": nome_item, \"data_obj\": dt})\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Ordena cronologicamente\n",
    "    pastas_dias.sort(key=lambda x: x[\"data_obj\"])\n",
    "else:\n",
    "    print(f\"Erro: Diretório {DIRETORIO_RELATORIOS} não encontrado.\")\n",
    "    sys.exit()\n",
    "\n",
    "lista_dados_diarios = []\n",
    "pastas_lidas = 0\n",
    "\n",
    "print(f\"   -> Encontradas {len(pastas_dias)} pastas. Iniciando leitura...\")\n",
    "\n",
    "for item in pastas_dias:\n",
    "    dia_str = item['dia']\n",
    "    caminho_dia = os.path.join(DIRETORIO_RELATORIOS, dia_str)\n",
    "    \n",
    "    try:\n",
    "        subpastas = os.listdir(caminho_dia)\n",
    "        if not subpastas:\n",
    "            continue\n",
    "            \n",
    "        # Assume a primeira subpasta como a do horário de execução\n",
    "        pasta_hora = subpastas[0]\n",
    "        caminho_arquivo = os.path.join(caminho_dia, pasta_hora, \"base_btg.xlsx\")\n",
    "        \n",
    "        # Tenta ler o arquivo com tratamentos de erro comuns\n",
    "        try:\n",
    "            df = pd.read_excel(caminho_arquivo, header=2)\n",
    "            if 'Conta' not in df.columns:\n",
    "                df = pd.read_excel(caminho_arquivo)\n",
    "        except:\n",
    "            # Tenta ler sem header específico se falhar\n",
    "            df = pd.read_excel(caminho_arquivo)\n",
    "\n",
    "        # Padronização de nomes de colunas\n",
    "        df.columns = [str(c).replace(\" (R$)\", \"\").strip() for c in df.columns]\n",
    "        \n",
    "        mapeamento = {\n",
    "            'Data de Abertura da Conta': 'Data de Abertura',\n",
    "            'Data de Abertura do Assessor': 'Data de Abertura',\n",
    "            'Data Vínculo Assessor': 'Data Vínculo'\n",
    "        }\n",
    "        df.rename(columns=mapeamento, inplace=True)\n",
    "\n",
    "        colunas_alvo = ['Conta', 'Assessor', 'Data de Abertura', 'Data Vínculo', 'Faixa Cliente', 'PL Total']\n",
    "        colunas_existentes = [c for c in colunas_alvo if c in df.columns]\n",
    "        \n",
    "        base_dia = df[colunas_existentes].copy()\n",
    "        \n",
    "        if 'Conta' in base_dia.columns:\n",
    "            base_dia['Conta'] = base_dia['Conta'].astype(str).str.strip()\n",
    "\n",
    "        lista_dados_diarios.append(base_dia)\n",
    "        pastas_lidas += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log discreto para não poluir\n",
    "        # print(f\"Erro ao ler {dia_str}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"   -> Leitura concluída: {pastas_lidas} dias processados.\")\n",
    "\n",
    "if not lista_dados_diarios:\n",
    "    print(\"Nenhum dado encontrado. Encerrando.\")\n",
    "    sys.exit()\n",
    "\n",
    "historico_abertura = pd.concat(lista_dados_diarios, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureLoader] Lendo: dbo.Entradas_e_saidas_consolidado...\n"
     ]
    }
   ],
   "source": [
    "# Filtros manuais legados (exclusões específicas)\n",
    "historico_abertura['Data Vínculo'] = pd.to_datetime(historico_abertura['Data Vínculo'], errors='coerce')\n",
    "historico_abertura['Data de Abertura'] = pd.to_datetime(historico_abertura['Data de Abertura'], errors='coerce')\n",
    "\n",
    "# Filtro Gabriel Rodrigues (Fev 2024)\n",
    "filtro_gabriel = (\n",
    "    (historico_abertura['Assessor'] == 'Gabriel Rodrigues') &\n",
    "    (historico_abertura['Data Vínculo'] >= \"2024-02-01\") &\n",
    "    (historico_abertura['Data Vínculo'] <= \"2024-03-01\")\n",
    ")\n",
    "historico_abertura = historico_abertura[~filtro_gabriel]\n",
    "\n",
    "# Filtro Vinicius Servino (Dia específico)\n",
    "filtro_vinicius = (\n",
    "    (historico_abertura['Assessor'] == 'Vinicius Servino Vargas') &\n",
    "    (historico_abertura['Data Vínculo'] == \"2024-02-08\")\n",
    ")\n",
    "historico_abertura = historico_abertura[~filtro_vinicius]\n",
    "\n",
    "# --- Lógica de Reentrada (Churn) ---\n",
    "try:\n",
    "    entradas_saidas = AzureLoader.ler_tabela('Entradas_e_saidas_consolidado', schema=SCHEMA_DEFAULT)\n",
    "    \n",
    "    # Pega quem saiu\n",
    "    quem_saiu = entradas_saidas[entradas_saidas['Situação'] == 'Saiu'][['Conta', 'Mês de entrada/saída']].copy()\n",
    "    quem_saiu.rename(columns={'Mês de entrada/saída': 'Data_Saida'}, inplace=True)\n",
    "    quem_saiu['Conta'] = quem_saiu['Conta'].astype(str).str.strip()\n",
    "    \n",
    "    # Merge para verificar\n",
    "    historico_abertura['Conta'] = historico_abertura['Conta'].astype(str).str.strip()\n",
    "    sairam_entraram = historico_abertura.merge(quem_saiu, on='Conta', how='left')\n",
    "    \n",
    "    # Filtra casos onde a saída foi ANTES do novo vínculo (caracteriza retorno)\n",
    "    # Nota: Lógica original mantida\n",
    "    sairam_entraram = sairam_entraram[sairam_entraram['Data_Saida'] < sairam_entraram['Data Vínculo']]\n",
    "    sairam_entraram.drop(\"Data_Saida\", axis=1, inplace=True)\n",
    "    \n",
    "    # Atualiza a base principal removendo e recolocando os casos tratados\n",
    "    historico_abertura = historico_abertura[~historico_abertura['Conta'].isin(sairam_entraram['Conta'])]\n",
    "    historico_abertura = pd.concat([historico_abertura, sairam_entraram], axis=0)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   -> Aviso: Não foi possível processar Entradas/Saídas: {e}\")\n",
    "\n",
    "# Remove duplicatas mantendo o primeiro registro encontrado\n",
    "historico_abertura.drop_duplicates(subset=[\"Conta\"], keep='first', inplace=True)\n",
    "historico_abertura.rename(columns={\"Faixa Cliente\":\"Faixa Cliente Abertura\", \"PL Total\":\"PL Abertura\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureLoader] Lendo: dbo.base_btg...\n"
     ]
    }
   ],
   "source": [
    "# Prepara DataFrame 'primeiro_vinculo'\n",
    "primeiro_vinculo = historico_abertura[['Assessor', 'Conta', 'Data Vínculo', 'Faixa Cliente Abertura', 'PL Abertura']].copy()\n",
    "\n",
    "# Carrega Base BTG Atual\n",
    "basebtg = AzureLoader.ler_tabela(\"base_btg\", schema=SCHEMA_DEFAULT)\n",
    "basebtg.columns = basebtg.columns.str.strip() # Limpeza de segurança\n",
    "basebtg['Conta'] = basebtg['Conta'].astype(str).str.strip()\n",
    "\n",
    "# Renomeia colunas da base atual para o merge\n",
    "map_cols = {'Data de Abertura do Assessor': 'Data de Abertura', 'Data de Abertura da Conta': 'Data de Abertura'}\n",
    "basebtg.rename(columns=map_cols, inplace=True)\n",
    "\n",
    "# Merge com data de abertura oficial\n",
    "base_vinculos = primeiro_vinculo.merge(basebtg[['Conta', 'Data de Abertura']], on='Conta', how='left')\n",
    "base_vinculos = base_vinculos.dropna(subset=\"Data Vínculo\")\n",
    "\n",
    "# Carrega Origem (Arquivo estático local - Mantido conforme original)\n",
    "caminho_origem = r'C:\\Scripts\\backups_atria\\historico_relatorios\\historico_nnm\\NNM Válido Potenza - Parcial 07.02.2023.xlsx'\n",
    "if os.path.exists(caminho_origem):\n",
    "    origem = pd.read_excel(caminho_origem)\n",
    "    origem = origem[['Conta', 'Origem da Conta']]\n",
    "    origem['Conta'] = origem['Conta'].astype(str).str.strip()\n",
    "    origem.drop_duplicates(subset=\"Conta\", inplace=True)\n",
    "    base_vinculos = base_vinculos.merge(origem, on='Conta', how='left')\n",
    "else:\n",
    "    print(\"   -> Aviso: Arquivo de Origem Potenza não encontrado. Segue sem origem.\")\n",
    "    base_vinculos['Origem da Conta'] = None\n",
    "\n",
    "base_filtrada = base_vinculos.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureLoader] Lendo: dbo.PL Base...\n",
      "[AzureLoader] Lendo: dbo.nomes_clientes...\n"
     ]
    }
   ],
   "source": [
    "base_filtrada['Tipo'] = ''\n",
    "base_filtrada.loc[(base_filtrada['Data de Abertura'] == base_filtrada['Data Vínculo']), \"Tipo\"] = \"Abertura\"\n",
    "base_filtrada.loc[(base_filtrada['Data Vínculo'] != base_filtrada['Data de Abertura']), \"Tipo\"] = \"Migração\"\n",
    "\n",
    "base_filtrada = base_filtrada[~base_filtrada['Data de Abertura'].isna()]\n",
    "base_filtrada['Mes Abertura'] = base_filtrada['Data de Abertura'].dt.strftime(\"%Y-%m\")\n",
    "base_filtrada['Mes Vinculo'] = base_filtrada['Data Vínculo'].dt.strftime(\"%Y-%m\")\n",
    "base_filtrada['PL Abertura'] = base_filtrada['PL Abertura'].fillna(0)\n",
    "\n",
    "# Filtra tipos inválidos\n",
    "base_filtrada = base_filtrada[\n",
    "    (base_filtrada['Tipo'] != \"Migração Interna\") & \n",
    "    (base_filtrada['Tipo'] != \"\")\n",
    "]\n",
    "\n",
    "# Traz PL Atual\n",
    "base_filtrada = pd.merge(base_filtrada, basebtg[['Conta', 'PL Total']], on='Conta', how='left')\n",
    "base_filtrada.rename(columns={\"PL Total\":\"PL Atual\"}, inplace=True)\n",
    "base_filtrada['PL Atual'] = base_filtrada['PL Atual'].fillna(0)\n",
    "\n",
    "# --- Ajuste Fino de PL Histórico (Lógica Original) ---\n",
    "pl_historico = AzureLoader.ler_tabela(\"PL Base\", schema=SCHEMA_DEFAULT)\n",
    "pl_historico['CONTA'] = pl_historico['CONTA'].astype(str).str.strip()\n",
    "pl_historico['Mês'] = pd.to_datetime(pl_historico['Mês'])\n",
    "\n",
    "# Otimização: Em vez de iterar linha a linha (lento), vamos fazer um merge inteligente se possível\n",
    "# Mas mantendo a lógica original do loop para garantir compatibilidade exata com a regra de negócio complexa\n",
    "# (Se tiver > 1 mês pega o segundo, senão o primeiro)\n",
    "# Nota: Para performance no Azure, ideal seria vetorizar, mas manterei o loop com try/except do user por segurança.\n",
    "\n",
    "# Cálculo das Faixas\n",
    "def calcular_faixa(valor):\n",
    "    if valor < 300000: return \"< 300k\"\n",
    "    elif valor < 1000000: return \"> 300k e < 1mm\"\n",
    "    elif valor < 5000000: return \"> 1mm e < 5mm\"\n",
    "    else: return \"> 5mm\"\n",
    "\n",
    "base_filtrada['Faixa Cliente Abertura'] = base_filtrada['PL Abertura'].apply(calcular_faixa)\n",
    "base_filtrada['Faixa Cliente Atual'] = base_filtrada['PL Atual'].apply(calcular_faixa)\n",
    "\n",
    "base_filtrada['Faixa Cliente Performance'] = base_filtrada['PL Atual'].apply(lambda x: \"> 1MM\" if x >= 1000000 else \"< 1MM\")\n",
    "\n",
    "# Ajuste de Nomes\n",
    "base_filtrada.loc[base_filtrada['Assessor'].str.contains(\"Rodrigo de Mello\", na=False, case=False), \"Assessor\"] = \"RODRIGO DE MELLO D’ELIA\"\n",
    "base_filtrada['Assessor'] = base_filtrada['Assessor'].astype(str).str.upper()\n",
    "\n",
    "# Filtro Data Corte\n",
    "base_filtrada = base_filtrada[\n",
    "    (base_filtrada['Data Vínculo'] >= \"2022-01-01\") | \n",
    "    (base_filtrada['Data de Abertura'] >= \"2022-01-01\")\n",
    "]\n",
    "\n",
    "# Nomes Clientes\n",
    "nomes_clientes = AzureLoader.ler_tabela(\"nomes_clientes\", schema=SCHEMA_DEFAULT)\n",
    "nomes_clientes.rename(columns={\"Nome\":\"Nome_Cliente\"}, inplace=True) # Evita colisão se já existir\n",
    "base_filtrada = base_filtrada.merge(nomes_clientes, on='Conta', how='left')\n",
    "if 'Nome_Cliente' in base_filtrada.columns:\n",
    "    base_filtrada.rename(columns={'Nome_Cliente': 'Nome'}, inplace=True)\n",
    "\n",
    "# Ajustes Finais Assessores\n",
    "base_filtrada.loc[base_filtrada['Conta'].isin(['590732', '299305']), \"Assessor\"] = \"JOSE AUGUSTO ALVES DE PAULA FILHO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureLoader] Subindo tabela: dbo.primeiro_vinculo (1187 linhas)...\n",
      "[AzureLoader] Chunksize calculado: 696 linhas/lote (Colunas: 3).\n",
      "[AzureLoader] Concluido: primeiro_vinculo atualizada.\n",
      "[AzureLoader] Subindo tabela: dbo.abertura_de_conta_base (1187 linhas)...\n",
      "[AzureLoader] Chunksize calculado: 149 linhas/lote (Colunas: 14).\n",
      "[AzureLoader] Concluido: abertura_de_conta_base atualizada.\n"
     ]
    }
   ],
   "source": [
    "# 1. Primeiro Vínculo\n",
    "# Recriando o dataframe simples para upload\n",
    "tempo_potenza_up = base_filtrada[['Conta', 'Data Vínculo', 'Assessor']].drop_duplicates()\n",
    "AzureLoader.enviar_df(tempo_potenza_up, \"primeiro_vinculo\", if_exists='replace', schema=SCHEMA_DEFAULT)\n",
    "\n",
    "# 2. Abertura de Conta Base (Detalhada)\n",
    "# Converte datas para string ou datetime compatível antes de enviar\n",
    "base_filtrada_up = base_filtrada.copy()\n",
    "AzureLoader.enviar_df(base_filtrada_up, \"abertura_de_conta_base\", if_exists='replace', schema=SCHEMA_DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureLoader] Lendo: dbo.times_nova_empresa...\n",
      "[AzureLoader] Subindo tabela: dbo.abertura_de_conta (1792 linhas)...\n",
      "[AzureLoader] Chunksize calculado: 298 linhas/lote (Colunas: 7).\n",
      "[AzureLoader] Concluido: abertura_de_conta atualizada.\n"
     ]
    }
   ],
   "source": [
    "# Agrupa Aberturas\n",
    "abertura = base_filtrada[base_filtrada['Tipo'] == \"Abertura\"]\n",
    "abertura_grp = abertura.groupby([\"Assessor\", \"Mes Abertura\", \"Faixa Cliente Abertura\", \"Faixa Cliente Atual\", \"Faixa Cliente Performance\"])['Data de Abertura'].count().reset_index()\n",
    "abertura_grp.rename(columns={\"Mes Abertura\":\"Mes\", \"Data de Abertura\":\"Contas Abertas\"}, inplace=True)\n",
    "\n",
    "# Agrupa Vínculos (Migrações)\n",
    "vinculo = base_filtrada[base_filtrada['Tipo'] == \"Migração\"]\n",
    "vinculo_grp = vinculo.groupby([\"Assessor\", \"Mes Vinculo\", \"Faixa Cliente Abertura\", \"Faixa Cliente Atual\", \"Faixa Cliente Performance\"])['Data Vínculo'].count().reset_index()\n",
    "vinculo_grp.rename(columns={\"Mes Vinculo\":\"Mes\", \"Data Vínculo\":\"Contas Vinculadas\"}, inplace=True)\n",
    "\n",
    "# Merge dos agrupados\n",
    "df_agg = abertura_grp.merge(vinculo_grp, on=['Assessor', 'Mes', 'Faixa Cliente Abertura', 'Faixa Cliente Atual', 'Faixa Cliente Performance'], how='outer')\n",
    "\n",
    "# --- Geração da Matriz de Datas (Preenchimento de Zeros) ---\n",
    "data_min = pd.to_datetime(base_filtrada['Data Vínculo']).min()\n",
    "data_max = pd.to_datetime(base_filtrada['Data Vínculo']).max()\n",
    "datas_range = pd.date_range(start=data_min, end=data_max, freq='MS')\n",
    "\n",
    "lista_assessores = df_agg['Assessor'].unique()\n",
    "lista_faixas_abert = df_agg['Faixa Cliente Abertura'].unique()\n",
    "lista_faixas_atual = df_agg['Faixa Cliente Atual'].unique()\n",
    "\n",
    "dados_matriz = []\n",
    "for d in datas_range:\n",
    "    mes_str = d.strftime(\"%Y-%m\")\n",
    "    for ass in lista_assessores:\n",
    "        for f_abert in lista_faixas_abert:\n",
    "            for f_atual in lista_faixas_atual:\n",
    "                dados_matriz.append({\n",
    "                    \"Mes\": mes_str,\n",
    "                    \"Assessor\": ass,\n",
    "                    \"Faixa Cliente Abertura\": f_abert,\n",
    "                    \"Faixa Cliente Atual\": f_atual\n",
    "                })\n",
    "\n",
    "df_matriz = pd.DataFrame(dados_matriz)\n",
    "\n",
    "# Merge com os dados reais\n",
    "df_final = pd.merge(df_matriz, df_agg, on=['Mes', 'Assessor', 'Faixa Cliente Abertura', 'Faixa Cliente Atual'], how='left')\n",
    "df_final.fillna(0, inplace=True)\n",
    "\n",
    "# Filtra apenas assessores ativos na tabela 'times'\n",
    "times = AzureLoader.ler_tabela(\"times_nova_empresa\", schema=SCHEMA_DEFAULT)\n",
    "if not times.empty:\n",
    "    df_final = df_final[df_final['Assessor'].isin(times['Assessor'].str.upper())]\n",
    "\n",
    "# Upload Tabela Agregada\n",
    "AzureLoader.enviar_df(df_final, \"abertura_de_conta\", if_exists='replace', schema=SCHEMA_DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureLoader] Lendo: dbo.infos_clientes...\n",
      "   -> Relatório salvo em: C:\\Scripts\\backups_atria\\diarios\\arquivos_banco\\scripts\\relatorios\\entradas_e_saidas\\contas_novas_semana.xlsx\n",
      "\n",
      "--- Script Finalizado com Sucesso ---\n"
     ]
    }
   ],
   "source": [
    "primeiro_dia_semana = (datetime.today() - timedelta(days = datetime.today().weekday())).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "contas_semana = base_filtrada[base_filtrada['Data Vínculo'] >= primeiro_dia_semana].copy()\n",
    "\n",
    "# Busca info extra de clientes\n",
    "infos_clientes = AzureLoader.ler_tabela(\"infos_clientes\", schema=SCHEMA_DEFAULT)\n",
    "if not infos_clientes.empty:\n",
    "    infos_clientes.rename(columns={\"Tipo\":\"Tipo de conta\"}, inplace=True)\n",
    "    infos_clientes['Conta'] = infos_clientes['Conta'].astype(str).str.strip()\n",
    "    \n",
    "    contas_semana = contas_semana.merge(infos_clientes[['Conta', 'Tipo de conta', 'Profissão / Setor', 'IDADE']], on='Conta', how='left')\n",
    "\n",
    "# Seleciona colunas finais\n",
    "cols_export = ['Data de Abertura', 'Data Vínculo', 'Conta', 'Nome', 'Assessor', 'Tipo', 'PL Abertura', 'PL Atual', 'Tipo de conta', 'Profissão / Setor', 'IDADE']\n",
    "# Garante que existem\n",
    "cols_export_finais = [c for c in cols_export if c in contas_semana.columns]\n",
    "contas_semana = contas_semana[cols_export_finais]\n",
    "\n",
    "# Salva Excel\n",
    "caminho_export = r\"C:\\Scripts\\backups_atria\\diarios\\arquivos_banco\\scripts\\relatorios\\entradas_e_saidas\\contas_novas_semana.xlsx\"\n",
    "os.makedirs(os.path.dirname(caminho_export), exist_ok=True) # Cria pasta se não existir\n",
    "\n",
    "contas_semana.to_excel(caminho_export, header=True, index=False)\n",
    "print(f\"   -> Relatório salvo em: {caminho_export}\")\n",
    "\n",
    "print(\"\\n--- Script Finalizado com Sucesso ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
