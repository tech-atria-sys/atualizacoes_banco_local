{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0b1557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy \n",
    "from sqlalchemy import text\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Adicionando caminhos dos módulos\n",
    "sys.path.insert(0, r'C:\\Scripts\\modules_azure\\database')\n",
    "sys.path.insert(0, r'C:\\Scripts\\modules_azure\\parameters')\n",
    "\n",
    "from connection_azure import Connect\n",
    "from azure_loader import AzureLoader\n",
    "from parametros import Parametros\n",
    "\n",
    "SCHEMA_DEFAULT = \"dbo\"\n",
    "CAMINHO_RELATORIOS = Parametros.caminho_completo_atual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de83606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CAMINHO_RELATORIOS:\n",
    "    print(\"Erro: Pasta de relatórios de hoje não encontrada.\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff654609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de datas\n",
    "data_hoje_str = datetime.today().strftime(\"%d-%m-%Y\")\n",
    "# Nota: O código original usava dias=7 na variável chamada 'quinze_dias'. Mantive a lógica original.\n",
    "data_corte = (datetime.today() - timedelta(days=7)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e100812",
   "metadata": {},
   "source": [
    "#### CAPTAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010c9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Busca dinâmica do arquivo nnm.xlsx\n",
    "    caminho_nnm = None\n",
    "    pastas_busca = [CAMINHO_RELATORIOS, os.path.dirname(CAMINHO_RELATORIOS)]\n",
    "    \n",
    "    for pasta in pastas_busca:\n",
    "        if os.path.exists(os.path.join(pasta, \"nnm.xlsx\")):\n",
    "            caminho_nnm = os.path.join(pasta, \"nnm.xlsx\")\n",
    "            break\n",
    "            \n",
    "    if not caminho_nnm:\n",
    "        raise FileNotFoundError(\"Arquivo 'nnm.xlsx' não encontrado.\")\n",
    "\n",
    "    captacao = pd.read_excel(caminho_nnm, header=2)\n",
    "    \n",
    "    # Tratamento\n",
    "    for coluna in captacao.columns:\n",
    "        captacao.rename(columns={coluna:coluna.upper()}, inplace=True)\n",
    "        \n",
    "    captacao['CONTA'] = captacao['CONTA'].astype(str).str.strip()\n",
    "    captacao.rename(columns={\"ASSESSOR\":'Assessor'}, inplace=True)\n",
    "    \n",
    "    # Ajustes de nomes\n",
    "    captacao.loc[captacao['Assessor'] == \"Rodrigo de Mello DElia\", \"Assessor\"] = \"Rodrigo de Mello D’Elia\"\n",
    "    captacao['Assessor'] = captacao['Assessor'].apply(lambda nome: str(nome).upper())\n",
    "    \n",
    "    captacao = captacao.loc[:,[\"CONTA\", \"MERCADO\", \"DESCRIÇÃO\", \"ATIVO\",\n",
    "                               \"CAPTAÇÃO\", \"DATA\", \"TIPO\", \"Assessor\"]]\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao ler NNM: {e}\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c560b3",
   "metadata": {},
   "source": [
    "##### OFFSHORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "481518bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    contas_offshore = pd.read_excel(r\"C:\\Scripts\\historico_auc\\AuC Offshore.xlsx\", sheet_name='AuC Offshore')\n",
    "    contas_offshore = contas_offshore.loc[:,[\"Conta\", \"Assessor\"]]\n",
    "    \n",
    "    offshore = pd.read_excel(r\"C:\\Scripts\\historico_nnm\\NNM Offshore.xlsx\", sheet_name=\"NNM Offshore\")\n",
    "    offshore['Conta'] = offshore['Conta'].astype(str).str.strip()\n",
    "    \n",
    "    offshore = offshore.merge(contas_offshore, on='Conta', how='left')\n",
    "    offshore = offshore[~offshore['Conta'].isnull()]\n",
    "    \n",
    "    offshore.rename(columns={\"Data NNM\":\"DATA\", \"Conta\":\"CONTA\", \"NNM BRL\":\"CAPTAÇÃO\"}, inplace=True)\n",
    "    if \"NNM USD\" in offshore.columns:\n",
    "        offshore.drop([\"NNM USD\"], axis=1, inplace=True)\n",
    "        \n",
    "    offshore['Assessor'] = [str(nome).upper() for nome in offshore['Assessor']]\n",
    "    offshore['DATA'] = pd.to_datetime(offshore['DATA'], format=\"%d/%m/%Y\")\n",
    "    offshore['DESCRIÇÃO'] = \"OFFSHORE\"\n",
    "    \n",
    "    # Concatenação\n",
    "    captacao = pd.concat([offshore, captacao])\n",
    "    captacao['Assessor'] = captacao['Assessor'].astype(str).apply(lambda nome: nome.upper())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro em Offshore: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030f259",
   "metadata": {},
   "source": [
    "##### MIGRAÇÕES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b65cd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureLoader] Lendo: dbo.migracoes_btg...\n",
      "Erro em Migrações: [Errno 2] No such file or directory: 'C:\\\\Scripts\\\\relatórios\\\\\\\\06-02-2026\\\\0848\\\\troca_assessor.xlsx'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Leitura do banco Azure\n",
    "    migracoes_btg = AzureLoader.ler_tabela(\"migracoes_btg\", schema=SCHEMA_DEFAULT)\n",
    "    \n",
    "    # Leitura do arquivo de troca de assessor\n",
    "    caminho_troca = os.path.join(os.path.dirname(CAMINHO_RELATORIOS), \"troca_assessor.xlsx\")\n",
    "    # Tenta na pasta do dia se não achar na raiz da data\n",
    "    if not os.path.exists(caminho_troca):\n",
    "         caminho_troca = os.path.join(CAMINHO_RELATORIOS, \"troca_assessor.xlsx\")\n",
    "\n",
    "    troca_assessores = pd.read_excel(caminho_troca)\n",
    "    \n",
    "    # Filtros de contas específicas e troca de assessor\n",
    "    contas_filtro = [\n",
    "        '590732', '299305', '5173757', '5152837', \n",
    "        '5149832', '5917705', '15296593'\n",
    "    ]\n",
    "    \n",
    "    migracoes_btg['CONTA'] = migracoes_btg['CONTA'].astype(str).str.strip()\n",
    "    \n",
    "    migracoes_btg = migracoes_btg[\n",
    "        (migracoes_btg['CONTA'].isin(troca_assessores['Conta'].astype(str))) |\n",
    "        (migracoes_btg['CONTA'].isin(contas_filtro))\n",
    "    ]\n",
    "    \n",
    "    captacao['TIPO DE CAPTACAO'] = \"Padrão\"\n",
    "    migracoes_btg['TIPO DE CAPTACAO'] = \"Migração BTG\"\n",
    "    migracoes_btg['DESCRIÇÃO'] = \"Migração BTG\"\n",
    "    migracoes_btg['MERCADO'] = \"Migração BTG\"\n",
    "    \n",
    "    # Consolidação\n",
    "    captacao = pd.concat([captacao, migracoes_btg], axis=0)\n",
    "    captacao = captacao[captacao['TIPO'] != \"RS\"]\n",
    "    captacao.dropna(subset='DATA', axis=0, inplace=True)\n",
    "    \n",
    "    captacao['CONTA'] = captacao['CONTA'].astype(str)\n",
    "    captacao['CAPTAÇÃO'] = captacao['CAPTAÇÃO'].astype(float)\n",
    "    captacao['DATA'] = pd.to_datetime(captacao['DATA'], errors='coerce') # Ajustado para evitar erros de formato\n",
    "    \n",
    "    captacao = captacao[['DATA', 'CONTA', 'CAPTAÇÃO', 'Assessor', 'TIPO DE CAPTACAO', 'MERCADO']]\n",
    "    \n",
    "    # Filtro de data recente (os últimos dias definidos)\n",
    "    captacao = captacao[captacao['DATA'] >= data_corte].sort_values(by='DATA', ascending=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro em Migrações: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269506e8",
   "metadata": {},
   "source": [
    "##### CAPTAÇÃO HISTORICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f5f36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureLoader] Lendo: dbo.captacao_historico...\n",
      "[AzureLoader] Lendo: dbo.primeiro_vinculo...\n",
      "[AzureLoader] Lendo: dbo.base_btg...\n",
      "[AzureLoader] Lendo: dbo.Entradas_e_saidas_consolidado...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Lê histórico do Azure\n",
    "    captacao_historico = AzureLoader.ler_tabela(\"captacao_historico\", schema=SCHEMA_DEFAULT)\n",
    "    \n",
    "    # Remove período que será reprocessado (substituição)\n",
    "    captacao_historico = captacao_historico[captacao_historico['DATA'] < data_corte]\n",
    "    \n",
    "    # Lê tabelas auxiliares\n",
    "    primeiro_vinculo = AzureLoader.ler_tabela(\"primeiro_vinculo\", schema=SCHEMA_DEFAULT)\n",
    "    primeiro_vinculo.rename(columns={\"Conta\":\"CONTA\"}, inplace=True)\n",
    "    \n",
    "    # Lê Base BTG do Azure (Substitui Bases.basebtg())\n",
    "    basebtg = AzureLoader.ler_tabela(\"base_btg\", schema=SCHEMA_DEFAULT)\n",
    "    \n",
    "    # Concatena novo período processado com histórico antigo\n",
    "    captacao_historico = pd.concat([captacao, captacao_historico], axis=0)\n",
    "    \n",
    "    # Identifica Situação (Ativo/Inativo)\n",
    "    lista_contas_ativas = set(basebtg['Conta'].astype(str))\n",
    "    captacao_historico['CONTA'] = captacao_historico['CONTA'].astype(str).str.strip()\n",
    "    \n",
    "    # Otimização com map/apply é mais rápida que loop for\n",
    "    captacao_historico['Situacao'] = captacao_historico['CONTA'].apply(\n",
    "        lambda x: \"Ativo\" if x in lista_contas_ativas else \"Inativo\"\n",
    "    )\n",
    "\n",
    "    # --- Lógica de Saída de Conta (Churn) ---\n",
    "    # Lê histórico diário de PL via SQL Query específica\n",
    "    db = Connect()\n",
    "    conn = db.connect_techdb()\n",
    "    query_pl = f'SELECT \"Conta\", \"PL Total\", \"Data\" FROM {SCHEMA_DEFAULT}.\"pl_historico_diario\"'\n",
    "    pl_historico = pd.read_sql(query_pl, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    pl_historico.rename(columns={\"Data\":\"DATA\", \"Conta\":\"CONTA\"}, inplace=True)\n",
    "    pl_historico['CONTA'] = pl_historico['CONTA'].astype(str).str.strip()\n",
    "    pl_historico['DATA'] = pd.to_datetime(pl_historico['DATA'])\n",
    "    pl_historico.sort_values(\"DATA\", inplace=True)\n",
    "    \n",
    "    contas_inativas = captacao_historico[captacao_historico['Situacao'] == \"Inativo\"].drop_duplicates(subset='CONTA')\n",
    "    \n",
    "    debitos = []\n",
    "    \n",
    "    # Loop de cálculo de churn\n",
    "    for _, row in contas_inativas.iterrows():\n",
    "        conta = row['CONTA']\n",
    "        assessor = row['Assessor']\n",
    "        situacao = row['Situacao']\n",
    "        \n",
    "        hist_conta = pl_historico[pl_historico['CONTA'] == conta]\n",
    "        \n",
    "        if not hist_conta.empty:\n",
    "            # Pega último PL e inverte sinal\n",
    "            valor_captacao = hist_conta.iloc[-1]['PL Total'] * -1\n",
    "            data_evento = hist_conta['DATA'].max()\n",
    "        else:\n",
    "            valor_captacao = 0\n",
    "            data_evento = pd.to_datetime(data_corte) # Fallback\n",
    "\n",
    "        debitos.append({\n",
    "            \"DATA\": data_evento,\n",
    "            \"CONTA\": conta,\n",
    "            \"CAPTAÇÃO\": valor_captacao,\n",
    "            \"Assessor\": assessor,\n",
    "            \"Situacao\": situacao\n",
    "        })\n",
    "    \n",
    "    if debitos:\n",
    "        debitos_df = pd.DataFrame(debitos)\n",
    "        debitos_df = debitos_df.drop_duplicates(subset='CONTA', keep='first')\n",
    "        debitos_df = debitos_df[debitos_df['DATA'] >= data_corte]\n",
    "        \n",
    "        # Cruzamento com Entradas e Saídas\n",
    "        entradas_e_saidas = AzureLoader.ler_tabela(\"Entradas_e_saidas_consolidado\", schema=SCHEMA_DEFAULT)\n",
    "        entradas_e_saidas = entradas_e_saidas[['Conta', 'Mês de entrada/saída']].copy()\n",
    "        entradas_e_saidas.rename(columns={\"Conta\":\"CONTA\"}, inplace=True)\n",
    "        entradas_e_saidas['CONTA'] = entradas_e_saidas['CONTA'].astype(str).str.strip()\n",
    "        entradas_e_saidas = entradas_e_saidas.drop_duplicates(\"CONTA\", keep='last')\n",
    "        \n",
    "        debitos_df = pd.merge(debitos_df, entradas_e_saidas, on='CONTA', how='left')\n",
    "        \n",
    "        # Ajuste de Datas\n",
    "        debitos_df['Mês de entrada/saída'] = pd.to_datetime(debitos_df['Mês de entrada/saída'], format=\"%d-%m-%Y\", errors='coerce')\n",
    "        debitos_df['DATA'] = debitos_df['Mês de entrada/saída'].fillna(debitos_df['DATA'])\n",
    "        \n",
    "        debitos_df.drop('Mês de entrada/saída', axis=1, inplace=True)\n",
    "        debitos_df = debitos_df.dropna(subset=['DATA'])\n",
    "        \n",
    "        debitos_df['TIPO DE CAPTACAO'] = \"Saída de conta\"\n",
    "        debitos_df['MERCADO'] = \"Saída de conta\"\n",
    "        \n",
    "        captacao_historico = pd.concat([captacao_historico, debitos_df], axis=0)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro em Histórico/Churn: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e47969",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Atualizar Assessor Atual usando base_btg\n",
    "    assessor_atual = basebtg[['Conta', 'Assessor']].copy()\n",
    "    assessor_atual.rename(columns={\"Conta\":\"CONTA\", \"Assessor\":\"Assessor atual\"}, inplace=True)\n",
    "    assessor_atual['CONTA'] = assessor_atual['CONTA'].astype(str).str.strip()\n",
    "    \n",
    "    captacao_historico = captacao_historico.merge(assessor_atual, on='CONTA', how='left')\n",
    "    \n",
    "    captacao_historico['Assessor atual'] = captacao_historico['Assessor atual'].fillna(captacao_historico['Assessor'])\n",
    "    captacao_historico.drop(\"Assessor\", axis=1, inplace=True)\n",
    "    captacao_historico.rename(columns={\"Assessor atual\":\"Assessor\"}, inplace=True)\n",
    "    \n",
    "    captacao_historico['DATA'] = pd.to_datetime(captacao_historico['DATA'])\n",
    "    \n",
    "    # Merge Nomes\n",
    "    nomes_clientes = pd.read_excel(r\"C:\\\\Scripts\\\\nomes_clientes\\Nomes_Clientes2.xlsx\")\n",
    "    nomes_clientes.rename(columns={\"Conta\":\"CONTA\"}, inplace=True)\n",
    "    nomes_clientes['CONTA'] = nomes_clientes['CONTA'].astype(str).str.strip()\n",
    "    nomes_clientes.drop_duplicates(\"CONTA\", inplace=True)\n",
    "    \n",
    "    if \"Nome\" in captacao_historico.columns:\n",
    "        captacao_historico.drop(\"Nome\", axis=1, inplace=True)\n",
    "        \n",
    "    captacao_historico = captacao_historico.merge(nomes_clientes, on='CONTA', how='left')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro nos tratamentos finais: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b581a4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joaob\\AppData\\Local\\Temp\\ipykernel_26164\\1528210914.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  captacao_historico['SITUACAO'] = captacao_historico['SITUACAO'].fillna(captacao_historico['SITUAÇÃO'])\n",
      "C:\\Users\\joaob\\AppData\\Local\\Temp\\ipykernel_26164\\1528210914.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  captacao_historico.drop('SITUAÇÃO', axis=1, inplace=True)\n",
      "C:\\Users\\joaob\\AppData\\Local\\Temp\\ipykernel_26164\\1528210914.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  captacao_historico['CONTA'] = captacao_historico['CONTA'].astype(str).str.strip()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureLoader] Subindo tabela: dbo.captacao_historico (69140 linhas)...\n",
      "[AzureLoader] Chunksize calculado: 190 linhas/lote (Colunas: 11).\n",
      "[AzureLoader] Concluido: captacao_historico atualizada.\n",
      "--- Processo de Captação Finalizado com Sucesso ---\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 1. Padronizar todas as colunas para MAIÚSCULO\n",
    "    # Isso resolve o conflito 'Tipo' vs 'TIPO' (ambos viram TIPO)\n",
    "    captacao_historico.columns = captacao_historico.columns.str.upper()\n",
    "\n",
    "    # 2. Remover colunas duplicadas\n",
    "    # Se existiam dois 'TIPO', esse comando remove o segundo e mantém o primeiro\n",
    "    captacao_historico = captacao_historico.loc[:, ~captacao_historico.columns.duplicated()]\n",
    "\n",
    "    # 3. Tratamento de Conflito: SITUACAO vs SITUAÇÃO (acentuação)\n",
    "    if 'SITUAÇÃO' in captacao_historico.columns:\n",
    "        if 'SITUACAO' in captacao_historico.columns:\n",
    "            # Se as duas existem, preenche a sem acento e apaga a com acento\n",
    "            captacao_historico['SITUACAO'] = captacao_historico['SITUACAO'].fillna(captacao_historico['SITUAÇÃO'])\n",
    "            captacao_historico.drop('SITUAÇÃO', axis=1, inplace=True)\n",
    "        else:\n",
    "            # Se só existe a com acento, renomeia\n",
    "            captacao_historico.rename(columns={'SITUAÇÃO': 'SITUACAO'}, inplace=True)\n",
    "\n",
    "    # 4. Garantir tipos de dados\n",
    "    captacao_historico['CONTA'] = captacao_historico['CONTA'].astype(str).str.strip()\n",
    "    \n",
    "    # 5. Envio Seguro\n",
    "    AzureLoader.enviar_df(\n",
    "        df=captacao_historico,\n",
    "        nome_tabela=\"captacao_historico\",\n",
    "        if_exists=\"replace\",\n",
    "        schema=SCHEMA_DEFAULT\n",
    "    )\n",
    "\n",
    "    print(\"--- Processo de Captação Finalizado com Sucesso ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro no Upload Final: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
