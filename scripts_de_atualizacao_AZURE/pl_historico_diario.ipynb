{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from sqlalchemy import text\n",
    "\n",
    "# --- Configuração dos Módulos ---\n",
    "sys.path.insert(0, r'C:\\Scripts\\modules_azure\\database')\n",
    "sys.path.insert(0, r'C:\\Scripts\\modules_azure\\parameters')\n",
    "\n",
    "from connection_azure import Connect\n",
    "from azure_loader import AzureLoader\n",
    "from parametros import Parametros\n",
    "\n",
    "# --- Configurações Globais ---\n",
    "SCHEMA_DEFAULT = \"dbo\"\n",
    "NOME_TABELA = \"pl_historico_diario\"\n",
    "DIRETORIO_RELATORIOS = r\"C:\\Scripts\\relatórios\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Última data no banco: 02/02/2026\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    db = Connect()\n",
    "    conn = db.connect_techdb()\n",
    "    \n",
    "    # Query otimizada para pegar apenas o valor máximo\n",
    "    query = text(f'SELECT MAX(\"Data\") FROM {SCHEMA_DEFAULT}.\"{NOME_TABELA}\"')\n",
    "    result = conn.execute(query).scalar()\n",
    "    \n",
    "    if result:\n",
    "        dt_max = pd.to_datetime(result)\n",
    "        print(f\"   -> Última data no banco: {dt_max.strftime('%d/%m/%Y')}\")\n",
    "    else:\n",
    "        dt_max = pd.to_datetime(\"2020-01-01\") # Data antiga padrão caso a tabela esteja vazia\n",
    "        print(\"   -> Tabela vazia ou sem datas. Iniciando carga total.\")\n",
    "        \n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao consultar banco: {e}\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Encontrados 4 dias novos para processar.\n"
     ]
    }
   ],
   "source": [
    "pastas_para_processar = []\n",
    "\n",
    "if os.path.exists(DIRETORIO_RELATORIOS):\n",
    "    # Itera sobre todas as pastas na raiz\n",
    "    for nome_pasta in os.listdir(DIRETORIO_RELATORIOS):\n",
    "        caminho_completo = os.path.join(DIRETORIO_RELATORIOS, nome_pasta)\n",
    "        \n",
    "        if os.path.isdir(caminho_completo):\n",
    "            try:\n",
    "                # Tenta converter o nome da pasta em data\n",
    "                data_pasta = datetime.strptime(nome_pasta, \"%d-%m-%Y\")\n",
    "                \n",
    "                # Se a data da pasta for MAIOR que a última do banco, adiciona na fila\n",
    "                if data_pasta > dt_max:\n",
    "                    pastas_para_processar.append({\n",
    "                        \"data\": data_pasta,\n",
    "                        \"caminho\": caminho_completo,\n",
    "                        \"dia_str\": nome_pasta\n",
    "                    })\n",
    "            except ValueError:\n",
    "                continue # Ignora pastas que não são datas\n",
    "\n",
    "    # Ordena cronologicamente para inserir na ordem correta\n",
    "    pastas_para_processar.sort(key=lambda x: x[\"data\"])\n",
    "else:\n",
    "    print(f\"Erro: Diretório {DIRETORIO_RELATORIOS} não encontrado.\")\n",
    "    sys.exit()\n",
    "\n",
    "if not pastas_para_processar:\n",
    "    print(\"   -> O banco já está atualizado com a data mais recente disponível.\")\n",
    "    sys.exit()\n",
    "\n",
    "print(f\"   -> Encontrados {len(pastas_para_processar)} dias novos para processar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [OK] 03-02-2026: 1196 registros processados.\n",
      "   [OK] 04-02-2026: 1105 registros processados.\n",
      "   [OK] 05-02-2026: 1186 registros processados.\n",
      "   [OK] 06-02-2026: 1187 registros processados.\n"
     ]
    }
   ],
   "source": [
    "dfs_para_upload = []\n",
    "\n",
    "for item in pastas_para_processar:\n",
    "    dia_str = item['dia_str']\n",
    "    caminho_dia = item['caminho']\n",
    "    \n",
    "    try:\n",
    "        # Busca subpastas (horários)\n",
    "        subpastas = [f for f in os.listdir(caminho_dia) if os.path.isdir(os.path.join(caminho_dia, f))]\n",
    "        \n",
    "        if not subpastas:\n",
    "            print(f\"   [Pular] {dia_str}: Nenhuma subpasta encontrada.\")\n",
    "            continue\n",
    "            \n",
    "        # Pega a primeira pasta de horário\n",
    "        pasta_horario = subpastas[0]\n",
    "        caminho_arquivo = os.path.join(caminho_dia, pasta_horario, \"base_btg.xlsx\")\n",
    "        \n",
    "        if not os.path.exists(caminho_arquivo):\n",
    "            print(f\"   [Pular] {dia_str}: Arquivo base_btg.xlsx não encontrado.\")\n",
    "            continue\n",
    "\n",
    "        # Leitura do Excel\n",
    "        try:\n",
    "            df = pd.read_excel(caminho_arquivo, header=2)\n",
    "            if \"Conta\" not in df.columns:\n",
    "                df = pd.read_excel(caminho_arquivo)\n",
    "        except:\n",
    "            df = pd.read_excel(caminho_arquivo)\n",
    "\n",
    "        # Limpeza de nomes de colunas\n",
    "        df.columns = [str(c).replace(\" (R$)\", \"\").strip() for c in df.columns]\n",
    "        \n",
    "        # Seleção de Colunas\n",
    "        colunas_desejadas = [\n",
    "            \"Conta\", \"Assessor\", \"PL Total\", \"Tipo\", \"Conta Corrente\", \n",
    "            \"Fundos\", \"Renda Fixa\", \"Renda Variável\", \"Previdência\", \"Derivativos\"\n",
    "        ]\n",
    "        \n",
    "        # Filtra apenas as que existem no arquivo\n",
    "        cols_existentes = [c for c in colunas_desejadas if c in df.columns]\n",
    "        df_final = df[cols_existentes].copy()\n",
    "        \n",
    "        # Adiciona colunas de Data\n",
    "        df_final['Data'] = item['data'] # Objeto datetime\n",
    "        df_final['Mês'] = item['data'].strftime(\"%Y-%m\")\n",
    "        \n",
    "        # Tratamento de Tipos\n",
    "        if 'Conta' in df_final.columns:\n",
    "            df_final['Conta'] = df_final['Conta'].astype(str).str.strip()\n",
    "            \n",
    "        # Preenche nulos numéricos com 0\n",
    "        cols_numericas = [\"PL Total\", \"Conta Corrente\", \"Fundos\", \"Renda Fixa\", \"Renda Variável\", \"Previdência\", \"Derivativos\"]\n",
    "        for col in cols_numericas:\n",
    "            if col in df_final.columns:\n",
    "                df_final[col] = pd.to_numeric(df_final[col], errors='coerce').fillna(0)\n",
    "\n",
    "        dfs_para_upload.append(df_final)\n",
    "        print(f\"   [OK] {dia_str}: {len(df_final)} registros processados.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   [ERRO] Falha ao ler dia {dia_str}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureLoader] Subindo tabela: dbo.pl_historico_diario (4674 linhas)...\n",
      "[AzureLoader] Chunksize calculado: 174 linhas/lote (Colunas: 12).\n",
      "[AzureLoader] Concluido: pl_historico_diario atualizada.\n"
     ]
    }
   ],
   "source": [
    "if dfs_para_upload:\n",
    "    df_consolidado = pd.concat(dfs_para_upload, ignore_index=True)\n",
    "    \n",
    "    # Tratamento final de nomes para Assessor (Opcional, mas recomendado para padronização)\n",
    "    if 'Assessor' in df_consolidado.columns:\n",
    "        df_consolidado['Assessor'] = df_consolidado['Assessor'].astype(str).str.upper()\n",
    "    \n",
    "    AzureLoader.enviar_df(\n",
    "        df=df_consolidado,\n",
    "        nome_tabela=NOME_TABELA,\n",
    "        if_exists='append', # IMPORTANTE: Append para manter o histórico\n",
    "        schema=SCHEMA_DEFAULT\n",
    "    )\n",
    "else:\n",
    "    print(\"   -> Nenhum dado válido foi gerado para upload.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
